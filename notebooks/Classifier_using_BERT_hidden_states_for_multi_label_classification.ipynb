{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6bIuH0oYiIJ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfCYkDhAe0yy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from scipy.special import expit as sigmoid\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from datasets import Dataset, load_dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mSG8hIwerMO"
      },
      "source": [
        "### Load dataset from huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc1NQnI7cMPH"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset('bhujith10/multi_class_classification_dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to tokenize the dataset"
      ],
      "metadata": {
        "id": "iYFuGvLQzNgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dataset(tokenizer, config, dataset_name='bhujith10/multi_class_classification_dataset'):\n",
        "  \"\"\"\n",
        "  Tokenizes and preprocesses a dataset\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "  tokenizer (Tokenizer): The tokenizer used to preprocess the dataset\n",
        "  config (object): Configuration object containing settings related to the tokenizer\n",
        "  dataset_name (str): The name or path of the dataset to load\n",
        "\n",
        "  Returns\n",
        "  =======\n",
        "  Tokenized and preprocessed dataset\n",
        "  \"\"\"\n",
        "\n",
        "  def tokenize(batch):\n",
        "    \"\"\"\n",
        "    Tokenizes a single batch of text data.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "    batch (dict): A dictionary containing a batch of text data.\n",
        "\n",
        "    Returns\n",
        "    =======\n",
        "    dict: A dictionary with tokenized data.\n",
        "    \"\"\"\n",
        "    return tokenizer(batch['text'], truncation=True, padding=True, max_length=config.max_length, return_tensors='pt')\n",
        "\n",
        "  ds = load_dataset(dataset_name)\n",
        "\n",
        "  ds_encoded = ds.map(tokenize, batched=True, batch_size=None)\n",
        "\n",
        "  for split in ds_encoded:\n",
        "    ds_encoded[split].set_format('torch')\n",
        "\n",
        "  # Convert the labels into float datatype\n",
        "  ds_encoded = ds_encoded.map(lambda x: {\"labels_f\": x[\"labels\"].to(torch.float)},remove_columns=[\"labels\"])\n",
        "  ds_encoded = ds_encoded.rename_column(\"labels_f\", \"labels\")\n",
        "\n",
        "  return ds_encoded"
      ],
      "metadata": {
        "id": "OdNsH50EwkEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to load model and tokenizer"
      ],
      "metadata": {
        "id": "T0RZUIqCzRZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer(config,\n",
        "                             add_pad_token=False,\n",
        "                             quantization=False,\n",
        "                             peft=False,\n",
        "                             load_model_for_sequence_classification=False):\n",
        "  \"\"\"\n",
        "  Loads a model and its tokenizer based on the provided configuration.\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "  config (object): Configuration object with model and tokenizer attributes.\n",
        "  add_pad_token (bool): Whether to add a padding token.\n",
        "  peft (bool): Whether to apply LORA or not.\n",
        "  quantization (bool): Whether to apply quantization.\n",
        "  load_model_for_sequence_classification (bool): Whether to load the model with classification head or not\n",
        "\n",
        "  Returns\n",
        "  =======\n",
        "  Loaded model and tokenizer.\n",
        "  \"\"\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(config.checkpoint)\n",
        "\n",
        "  # Llama version 3 models already have a padding token\n",
        "  # Hence we need not add a padding token\n",
        "  if add_pad_token:\n",
        "      if 'Llama' in tokenizer.name_or_path:\n",
        "          tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
        "      else:\n",
        "          tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
        "\n",
        "  # I faced some errors while right padding in Mistral models\n",
        "  # Hence set padding_side as left for Mistral models alone\n",
        "  if 'Mistral' in tokenizer.name_or_path:\n",
        "      tokenizer.padding_side = \"left\"\n",
        "  else:\n",
        "      tokenizer.padding_side = \"right\"\n",
        "\n",
        "  if load_model_for_sequence_classification:\n",
        "      # Load the model with classification head\n",
        "      # num_labels specifies the number of neurons in the output layer\n",
        "      model =  AutoModelForSequenceClassification.from_pretrained(\n",
        "          pretrained_model_name_or_path=config.checkpoint,\n",
        "          quantization_config=quantization_config(config) if quantization else None,\n",
        "          torch_dtype=torch.bfloat16 if config.bf16 else torch.float16,\n",
        "          num_labels=config.num_labels,\n",
        "          problem_type=config.problem_type\n",
        "      )\n",
        "\n",
        "  else:\n",
        "      model = AutoModel.from_pretrained(checkpoint)\n",
        "\n",
        "  if add_pad_token:\n",
        "      model.config.pad_token_id = tokenizer.pad_token_id\n",
        "      if 'Llama' not in tokenizer.name_or_path:\n",
        "          model.resize_token_embeddings(len(tokenizer))\n",
        "  if peft:\n",
        "      peft_config = LoraConfig(\n",
        "          task_type=TaskType.SEQ_CLS,\n",
        "          r=config.lora_rank,\n",
        "          lora_alpha=config.lora_alpha,\n",
        "          lora_dropout=config.lora_dropout,\n",
        "          bias=config.lora_bias,\n",
        "          #target_modules=[\"q_proj\", \"k_proj\"]\n",
        "      )\n",
        "\n",
        "      model = get_peft_model(model, peft_config)\n",
        "\n",
        "  return model, tokenizer"
      ],
      "metadata": {
        "id": "ZEUj-gIkyNbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UymUZp9XQAD"
      },
      "source": [
        "### Function to extract hidden states from model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diaV4CyqPXOL"
      },
      "outputs": [],
      "source": [
        "def extract_hidden_states(batch, **kwargs):\n",
        "    \"\"\"\n",
        "    Extracts the hidden states from the model for the given batch of inputs.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "    batch (dict): A dictionary containing input tensors such as 'input_ids' and 'attention_mask'.\n",
        "    **kwargs: Additional keyword arguments, including:\n",
        "        - model : The pre-trained transformer model.\n",
        "        - tokenizer : The tokenizer used for the model.\n",
        "\n",
        "    Returns\n",
        "    =======\n",
        "    dict: A dictionary containing the hidden states (extracted from the first token (CLS) of the last layer).\n",
        "    \"\"\"\n",
        "    # Extract the model and tokenizer from kwargs\n",
        "    model = kwargs.get(\"model\")\n",
        "    tokenizer = kwargs.get(\"tokenizer\")\n",
        "\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    # Prepare input tensors for the model\n",
        "    inputs = {k: v.to(\"cuda\") for k, v in batch.items() if k in tokenizer.model_input_names}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Extract the last hidden state from the model output\n",
        "        last_hidden_state = model(**inputs).last_hidden_state\n",
        "\n",
        "    # Return the CLS token's hidden state as a NumPy array\n",
        "    return {\"hidden_state\": last_hidden_state[:, 0].cpu().numpy()}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to calculate micro and macro F1 scores"
      ],
      "metadata": {
        "id": "FAIJUWBMzvBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_f1_score(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates micro and macro F1-scores given the predicted and actual labels\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "  y_true (numpy array): Actual labels\n",
        "  y_pred (numpy array): Predicted labels\n",
        "\n",
        "  Returns\n",
        "  =======\n",
        "  dict: A dictionary containing micro f1 and macro f1 scores.\n",
        "  \"\"\"\n",
        "  # Generate a classification report to compute detailed metrics\n",
        "  clf_dict = classification_report(\n",
        "      y_true,\n",
        "      y_pred,\n",
        "      zero_division=0,\n",
        "      output_dict=True\n",
        "  )\n",
        "\n",
        "  return {\n",
        "      \"micro f1\": clf_dict[\"micro avg\"][\"f1-score\"],\n",
        "      \"macro f1\": clf_dict[\"macro avg\"][\"f1-score\"]\n",
        "  }"
      ],
      "metadata": {
        "id": "gzdlooWxG7kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to build classifier using hidden states"
      ],
      "metadata": {
        "id": "kFVpsZdhzxeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_using_hidden_states(ds, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Builds and trains a multi-label classifier using hidden states extracted from a transformer model.\n",
        "\n",
        "    Parameters\n",
        "    ==========\n",
        "    ds (DatasetDict): A Hugging Face DatasetDict containing train and test splits.\n",
        "    model: The pre-trained transformer model used to extract hidden states.\n",
        "    tokenizer: The tokenizer used for the model.\n",
        "\n",
        "    Returns\n",
        "    =======\n",
        "    dict: F1 score results containing micro f1 and macro f1 scores.\n",
        "    float: Time taken (in seconds) to predict on the test set.\n",
        "    \"\"\"\n",
        "    # Extract hidden states for the training set\n",
        "    train_ds = ds['train'].map(\n",
        "        extract_hidden_states,\n",
        "        batched=True,\n",
        "        batch_size=4,\n",
        "        fn_kwargs={\"model\": model, \"tokenizer\": tokenizer}\n",
        "    )\n",
        "\n",
        "    # Prepare training data (features and labels)\n",
        "    x_train = np.array(train_ds[\"hidden_state\"])\n",
        "    y_train = np.array(train_ds[\"labels\"])\n",
        "\n",
        "    random_forest_clf = RandomForestClassifier(n_estimators=500)\n",
        "\n",
        "    # Use One-vs-Rest strategy for multi-label classification\n",
        "    multi_class_clf = OneVsRestClassifier(random_forest_clf)\n",
        "    multi_class_clf.fit(x_train, y_train)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Extract hidden states for the test set\n",
        "    test_ds = ds['test'].map(\n",
        "        extract_hidden_states,\n",
        "        batched=True,\n",
        "        batch_size=4,\n",
        "        fn_kwargs={\"model\": model, \"tokenizer\": tokenizer}\n",
        "    )\n",
        "\n",
        "    x_test = np.array(test_ds[\"hidden_state\"])\n",
        "    y_test = np.array(test_ds[\"labels\"])\n",
        "\n",
        "    y_pred = multi_class_clf.predict(x_test)\n",
        "\n",
        "    # Calculate F1 scores for the test set predictions\n",
        "    f1_score_results = calculate_f1_score(y_test, y_pred)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    time_for_test_set_prediction = end_time - start_time\n",
        "\n",
        "    return f1_score_results, time_for_test_set_prediction"
      ],
      "metadata": {
        "id": "OPNtotmiCp9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to update model related info in config"
      ],
      "metadata": {
        "id": "gbXjJVdtFrlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_model_related_settings(checkpoint, config):\n",
        "  config.checkpoint = checkpoint\n",
        "  model_name = checkpoint.split('/')[-1]\n",
        "  currtime = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
        "  config.model_name = f\"{model_name}_{currtime}\"\n",
        "  config.local_save_path = config.model_name\n",
        "  return config"
      ],
      "metadata": {
        "id": "Nv46igdbFqu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build classifier that uses hidden states from bert model"
      ],
      "metadata": {
        "id": "kAagOB5fz5Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "  checkpoint:str = \"microsoft/deberta-v3-base\"\n",
        "  max_length:int = 512\n",
        "  num_labels:int = 6\n",
        "  problem_type:str = \"multi_label_classification\"\n",
        "  lora_rank:int = 8\n",
        "  lora_alpha:int = 32\n",
        "  lora_dropout:float = 0.1\n",
        "  lora_bias:str = \"none\"\n",
        "  device:str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  repo_user_id:str = \"bhujith10\"\n",
        "  model_name:str = \"\"\n",
        "  local_save_path:str = \"\"\n",
        "  bf16:bool = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "  fp16:bool = torch.cuda.is_available() and not torch.cuda.is_bf16_supported()\n",
        "  load_in_4bit:bool = True,\n",
        "  bnb_4bit_quant_type:bool = \"nf4\",\n",
        "  bnb_4bit_compute_dtype:bool = \"float16\",\n",
        "  bnb_4bit_use_double_quant:bool = False,\n",
        "  num_train_epochs:int = 2,\n",
        "  batch_size:int = 8,\n",
        "  gradient_accumulation_steps:int = 2,\n",
        "  gradient_checkpointing:bool = True\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "oyaI4OjWqLOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = 'google-bert/bert-large-uncased'\n",
        "config = update_model_related_settings(checkpoint, config)\n",
        "\n",
        "# Load the model without any classification head\n",
        "model, tokenizer = load_model_and_tokenizer(config=config,\n",
        "                                            add_pad_token=False,\n",
        "                                            peft=False,\n",
        "                                            load_model_for_sequence_classification=False\n",
        "                                            )\n",
        "\n",
        "ds = tokenize_dataset(tokenizer,\n",
        "                      config)\n",
        "\n",
        "# Build the classifier using BERT model hidden states\n",
        "f1_score_results, time_taken = build_classifier_using_hidden_states(ds, model, tokenizer)"
      ],
      "metadata": {
        "id": "jruOZTpvHeY-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}