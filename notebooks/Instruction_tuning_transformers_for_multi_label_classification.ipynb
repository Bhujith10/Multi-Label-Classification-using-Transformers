{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwKyoBuGY35k"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.46.3 accelerate scikit-multilearn peft datasets sentence-transformers bitsandbytes trl protobuf wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5EY3268Y35l"
      },
      "outputs": [],
      "source": [
        "!pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgAXLnZ9Y35l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from scipy.special import expit as sigmoid\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from trl import SFTTrainer\n",
        "from trl.trainer import SFTConfig\n",
        "from datasets import Dataset, load_dataset, DatasetDict\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorWithPadding, DataCollatorForLanguageModeling, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keys\n",
        "\n",
        "We need HuggingFace access tokens to upload the fine-tuned models to the HuggingFace repository and Weights & Biases (WandB) API keys to record the training metrics in WandB."
      ],
      "metadata": {
        "id": "SrWe_mg_ueUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrrmntO7Y35l"
      },
      "outputs": [],
      "source": [
        "os.environ[\"HF_TOKEN\"]=\"\"\n",
        "os.environ[\"WANDB_API_KEY\"]=\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T08:00:39.936119Z",
          "iopub.status.busy": "2024-11-30T08:00:39.935748Z",
          "iopub.status.idle": "2024-11-30T08:00:39.941531Z",
          "shell.execute_reply": "2024-11-30T08:00:39.940588Z",
          "shell.execute_reply.started": "2024-11-30T08:00:39.936084Z"
        },
        "id": "z__38QswY35l"
      },
      "outputs": [],
      "source": [
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to create Quantization config"
      ],
      "metadata": {
        "id": "HuG42y5CvH6t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5YCgciHY35l"
      },
      "outputs": [],
      "source": [
        "def quantization_config(config):\n",
        "  \"\"\"\n",
        "  Creates a configuration for 4-bit quantization of a model.\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "  config (object) : Configuration object with attributes.\n",
        "\n",
        "  Returns\n",
        "  =======\n",
        "  BitsAndBytesConfig: Configuration for 4-bit quantization.\n",
        "  \"\"\"\n",
        "  bnb_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=config.load_in_4bit,\n",
        "      bnb_4bit_quant_type=config.bnb_4bit_quant_type,\n",
        "      bnb_4bit_compute_dtype=config.bnb_4bit_compute_dtype,\n",
        "      bnb_4bit_use_double_quant=config.bnb_4bit_use_double_quant,\n",
        "  )\n",
        "  return bnb_config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to update model related settings"
      ],
      "metadata": {
        "id": "Il4Og5APvRQW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQP9O6MdY35l"
      },
      "outputs": [],
      "source": [
        "def update_model_related_settings(checkpoint, config):\n",
        "  \"\"\"\n",
        "  Updates configuration settings related to saving the model, including checkpoint and naming.\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "  checkpoint (str): Path to the model checkpoint.\n",
        "  config (object): Configuration object to be updated.\n",
        "\n",
        "  Returns\n",
        "  =======\n",
        "  config (object): Updated configuration object.\n",
        "  \"\"\"\n",
        "  model_name = f\"{checkpoint.split('/')[-1]}_instruction_tuned\"\n",
        "  config.checkpoint = checkpoint\n",
        "  currtime = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
        "  config.model_name = f\"{model_name}_{currtime}\"\n",
        "  config.local_save_path = config.model_name\n",
        "  return config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to calculate model size"
      ],
      "metadata": {
        "id": "OWxn4yCavWYE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fDpt-rRY35l"
      },
      "outputs": [],
      "source": [
        "def calculate_model_size(model):\n",
        "  \"\"\"\n",
        "  Calculates the total, trainable, and memory size of model parameters.\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "  model: The model whose parameters are to be calculated.\n",
        "\n",
        "  Returns\n",
        "  =======\n",
        "  tuple: Total parameters, trainable parameters, and memory size in MB.\n",
        "  \"\"\"\n",
        "  total_params = sum(p.numel() for p in model.parameters())\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  param_size_in_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
        "  param_size_in_mb = param_size_in_bytes / (1024 ** 2)\n",
        "\n",
        "  return total_params, trainable_params, param_size_in_mb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to return the subjects given the label encodings"
      ],
      "metadata": {
        "id": "57zN-HKQv71e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMTls7NrY35m"
      },
      "outputs": [],
      "source": [
        "def return_subjects(labels):\n",
        "  \"\"\"\n",
        "  Maps binary labels to their corresponding subjects and returns the list of selected subjects.\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "  labels (list): A binary list (1 or 0) indicating whether a subject is relevant (1) or not (0).\n",
        "\n",
        "  Returns\n",
        "  =======\n",
        "  list: A list of selected subjects corresponding to labels with a value of 1.\n",
        "  \"\"\"\n",
        "  subjects = ['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']\n",
        "\n",
        "  selected_subjects = [subject for subject, label  in zip(subjects, labels) if label == 1]\n",
        "  return selected_subjects"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to format the text for finetuning"
      ],
      "metadata": {
        "id": "JrrO1rCVwI8o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ8OB2AbY35m"
      },
      "outputs": [],
      "source": [
        "def format_text(sample, tokenizer):\n",
        "  \"\"\"\n",
        "  Formats the input sample for a language model using a chat-based template.\n",
        "  It includes system instructions, an example interaction, and the user's input.\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "  sample (dict): A dictionary containing the sample data, including 'text' (title and abstract) and 'labels'.\n",
        "  tokenizer: A tokenizer object used for formatting text for language model input.\n",
        "\n",
        "  Returns\n",
        "  =======\n",
        "  dict: The updated sample dictionary with the formatted text added as 'formatted_text'.\n",
        "  \"\"\"\n",
        "  system_content = \"\"\"\n",
        "  Given the title and abstract of a research paper, classify it into one or more of the following subjects based on its content: ['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance'].\n",
        "\n",
        "  Output Requirements:\n",
        "\n",
        "  Return only the most appropriate subjects (1 to 3) from the given list.\n",
        "  Do not include subjects outside the provided list.\n",
        "  Avoid selecting all subjects; focus on those most relevant to the paper's content.\n",
        "\n",
        "  RETURN ONLY A LIST AND NOTHING ELSE\n",
        "  \"\"\"\n",
        "  user_content = sample['text']\n",
        "\n",
        "  example_user_input = f\"\"\"\n",
        "  Title: Efficient methods for computing integrals in electronic structure calculations,\n",
        "  Abstract: Efficient methods are proposed, for computing integrals appeaing in electronic structure calculations.\n",
        "  The methods consist of two parts: the first part is to represent the integrals as contour integrals and the second one is to evaluate the contour integrals by the Clenshaw-Curtis quadrature.\n",
        "  The efficiency of the proposed methods is demonstrated through numerical experiments.\n",
        "  \"\"\"\n",
        "  example_user_output=f\"\"\"\n",
        "  ['Physics']\n",
        "  \"\"\"\n",
        "\n",
        "  if 'mistralai' in tokenizer.name_or_path:\n",
        "\n",
        "      assistant_content = str(return_subjects(sample['labels'])) + '\\t' + tokenizer.eos_token\n",
        "\n",
        "      user_content = system_content + \"\\n\" + example_user_input + \"\\n\" + example_user_output + \"\\n\" + user_content\n",
        "\n",
        "      messages = [\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": user_content\n",
        "          },\n",
        "          {\n",
        "              \"role\":\"assistant\",\n",
        "              \"content\":assistant_content\n",
        "          }\n",
        "      ]\n",
        "\n",
        "  else:\n",
        "      assistant_content = str(return_subjects(sample['labels'])) + '\\t' + '<|end_of_text|>'\n",
        "      messages = [\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": system_content,\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": example_user_input\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"assistant\",\n",
        "              \"content\": example_user_output\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": user_content\n",
        "          },\n",
        "          {\n",
        "              \"role\":\"assistant\",\n",
        "              \"content\":assistant_content\n",
        "          }\n",
        "      ]\n",
        "\n",
        "\n",
        "  sample[\"formatted_text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "  return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to format text for inference"
      ],
      "metadata": {
        "id": "UH4izx7R3ZTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_text_for_inference(sample, tokenizer):\n",
        "  \"\"\"\n",
        "  Formats the input sample for a language model using a chat-based template.\n",
        "  It includes system instructions, an example interaction, and the user's input.\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "  sample (dict): A dictionary containing the sample data, including 'text' (title and abstract) and 'labels'.\n",
        "  tokenizer: A tokenizer object used for formatting text for language model input.\n",
        "\n",
        "  Returns\n",
        "  =======\n",
        "  dict: The input sample augmented with a 'formatted_text' key containing the processed text.\n",
        "  \"\"\"\n",
        "\n",
        "  # Instruction prompt\n",
        "  system_content = f\"\"\"\n",
        "  Given the title and abstract of a research paper, classify it into one or more of the following subjects based on its content: ['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance'].\n",
        "\n",
        "  Output Requirements:\n",
        "\n",
        "  Return only the most appropriate subjects (1 to 3) from the given list.\n",
        "  Do not include subjects outside the provided list.\n",
        "  Avoid selecting all subjects; focus on those most relevant to the paper's content.\n",
        "  You are provided with an example\n",
        "\n",
        "  RETURN ONLY A LIST AND NOTHING ELSE\n",
        "  \"\"\"\n",
        "\n",
        "  # The actual text for classification\n",
        "  user_content = sample['text']\n",
        "\n",
        "  # Input and output for better context to the model\n",
        "  example_user_input = f\"\"\"\n",
        "  Title: Efficient methods for computing integrals in electronic structure calculations,\n",
        "  Abstract: Efficient methods are proposed, for computing integrals appearing in electronic structure calculations.\n",
        "  The methods consist of two parts: the first part is to represent the integrals as contour integrals and the second one is to evaluate the contour integrals by the Clenshaw-Curtis quadrature.\n",
        "  The efficiency of the proposed methods is demonstrated through numerical experiments.\n",
        "  \"\"\"\n",
        "  example_user_output = f\"\"\"\n",
        "  ['Physics']\n",
        "  \"\"\"\n",
        "\n",
        "  # Handle different tokenizer types\n",
        "  if 'mistralai' in tokenizer.name_or_path:\n",
        "      # For Mistral tokenizers\n",
        "      user_content = system_content + \"\\n\" + example_user_input + \"\\n\" + example_user_output + \"\\n\" + user_content\n",
        "      messages = [\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": user_content\n",
        "          }\n",
        "      ]\n",
        "  else:\n",
        "      # For other tokenizers, use a different template\n",
        "      messages = [\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": system_content,\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": example_user_input\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"assistant\",\n",
        "              \"content\": example_user_output\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": user_content\n",
        "          }\n",
        "      ]\n",
        "\n",
        "  # Apply the chat template to the sample using the tokenizer\n",
        "  sample[\"formatted_text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "  return sample"
      ],
      "metadata": {
        "id": "JEaO6vZx3YPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to return encodings given the subjects"
      ],
      "metadata": {
        "id": "mVEdRDwswQIM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHN3WU2mY35m"
      },
      "outputs": [],
      "source": [
        "def return_encodings_from_subjects(predicted_subjects):\n",
        "  \"\"\"\n",
        "  Converts a list of predicted subjects into a binary encoding based on predefined subject categories.\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "  predicted_subjects (list): A list of subject names predicted from a model.\n",
        "\n",
        "  Returns\n",
        "  =======\n",
        "  predicted_encodings (list): A binary list of length 6 where each element indicates the presence (1) or absence (0)\n",
        "        of a subject in the `predicted_subjects` list, corresponding to a particular order:\n",
        "  \"\"\"\n",
        "  subjects = ['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']\n",
        "  predicted_encodings = [1 if subject in predicted_subjects else 0 for subject in subjects]\n",
        "  return predicted_encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to extract the predictions list from raw responses"
      ],
      "metadata": {
        "id": "T0ZNPtthwXkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_list_from_raw_response(text):\n",
        "  \"\"\"\n",
        "  Extracts a list of predicted subjects from a raw text response and returns their binary encoding.\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "  text (str): A string containing a raw response that may include a list of predicted subjects\n",
        "              enclosed in square brackets.\n",
        "\n",
        "  Returns\n",
        "  =======\n",
        "  predicted_encodings (list): A binary list of length 6 encoding the presence or absence of each predefined subject\n",
        "        in the extracted list of predicted subjects. If no valid list is found in the text, returns [0, 0, 0, 0, 0, 0].\n",
        "  \"\"\"\n",
        "  # Regex pattern to match the last list\n",
        "  pattern = r\"\\[(?:'[^']*'(?:, )?)*\\]\"\n",
        "  match = re.search(pattern, text)\n",
        "\n",
        "  predicted_encodings = [0, 0, 0, 0, 0, 0]\n",
        "\n",
        "  if match:\n",
        "      predicted_subjects = ast.literal_eval(match.group())\n",
        "      predicted_encodings = return_encodings_from_subjects(predicted_subjects)\n",
        "\n",
        "  return predicted_encodings"
      ],
      "metadata": {
        "id": "yUZwDM7EwWuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to load the model and tokenizer from HuggingFace Repo given the name of the model"
      ],
      "metadata": {
        "id": "ilKNiX5swdAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG44DhN9Y35m"
      },
      "outputs": [],
      "source": [
        "def load_model_and_tokenizer(config,\n",
        "                             add_pad_token=True,\n",
        "                             use_cache=False,\n",
        "                             quantization=False):\n",
        "  \"\"\"\n",
        "  Loads a model and its tokenizer based on the provided configuration.\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "  config (object): Configuration object with model and tokenizer attributes.\n",
        "  add_pad_token (bool): Whether to add a padding token.\n",
        "  use_cache (bool): Whether to enable caching for the model.\n",
        "  quantization (bool): Whether to apply quantization.\n",
        "\n",
        "  Returns\n",
        "  =======\n",
        "  Loaded model and tokenizer.\n",
        "  \"\"\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(config.checkpoint, add_prefix_space=True)\n",
        "\n",
        "  # Llama provides a default padding token. We just have to set it.\n",
        "  # In case of other models, we have to add the pad tokens\n",
        "  if add_pad_token:\n",
        "      if 'Llama' in config.checkpoint:\n",
        "          tokenizer.pad_token='<|finetune_right_pad_id|>'\n",
        "          tokenizer.padding_side = 'right'\n",
        "      else:\n",
        "          tokenizer.add_special_tokens({\"pad_token\":'<pad>'})\n",
        "          tokenizer.padding_side = 'left'\n",
        "\n",
        "  # use_cache can be set to False while finetuning\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      config.checkpoint,\n",
        "      attn_implementation=\"flash_attention_2\",\n",
        "      use_cache=use_cache,\n",
        "      device_map=config.device_map,\n",
        "      quantization_config=quantization_config(config) if quantization else None,\n",
        "      torch_dtype=torch.bfloat16 if config.bf16 else torch.float16,\n",
        "      trust_remote_code=True\n",
        "      )\n",
        "\n",
        "  # In case, a new token was created, this change has to be made in model config also\n",
        "  if add_pad_token:\n",
        "      model.config.pad_token_id = tokenizer.pad_token_id\n",
        "      if 'Llama' not in config.checkpoint:\n",
        "          model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "  return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to finetune the model"
      ],
      "metadata": {
        "id": "Ig2zJn_4wmej"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgBSlqD0Y35m"
      },
      "outputs": [],
      "source": [
        "def finetune_model(model,tokenizer,dataset,config,peft=False,dataset_text_field='text'):\n",
        "  \"\"\"\n",
        "  Fine-tunes a model using the provided dataset and configuration.\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "  model: The pre-trained model to fine-tune.\n",
        "  tokenizer: The tokenizer for the model.\n",
        "  dataset: The dataset containing 'train' and 'val' splits.\n",
        "  config: Configuration object with training parameters.\n",
        "  peft (bool): Whether to apply parameter-efficient fine-tuning (LoRA).\n",
        "  dataset_text_field (str): Field in the dataset containing the text data.\n",
        "\n",
        "  Returns\n",
        "  =======\n",
        "  Fine-tuned model.\n",
        "  \"\"\"\n",
        "  num_train_epochs = config.num_train_epochs\n",
        "  train_batch_size = config.batch_size\n",
        "  gradient_accumulation_steps = config.gradient_accumulation_steps\n",
        "  max_steps = int((len(dataset['train'])*num_train_epochs)/(train_batch_size*gradient_accumulation_steps))\n",
        "\n",
        "  if peft:\n",
        "      # Load LoRA configuration\n",
        "      peft_config = LoraConfig(\n",
        "          r=config.lora_rank,\n",
        "          lora_alpha=config.lora_alpha,\n",
        "          lora_dropout=config.lora_dropout,\n",
        "          bias=config.lora_bias,\n",
        "          task_type=\"CAUSAL_LM\",\n",
        "          #target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "      )\n",
        "\n",
        "  training_arguments = SFTConfig(\n",
        "      max_steps=max_steps,\n",
        "      per_device_train_batch_size=train_batch_size,\n",
        "      gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "      gradient_checkpointing=True,\n",
        "        learning_rate=3e-4,\n",
        "        fp16=config.fp16,\n",
        "        bf16=config.bf16,\n",
        "        output_dir=f\"{config.model_name}_outputs\",\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        eval_steps=100,\n",
        "        save_steps=max_steps-100,\n",
        "        logging_steps=100,\n",
        "        save_strategy=\"steps\",\n",
        "        evaluation_strategy=\"steps\",\n",
        "        warmup_ratio=0.02,\n",
        "        report_to=\"wandb\",\n",
        "        run_name=f\"{config.repo_user_id}/{config.model_name}_results\",\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        dataset_batch_size=4,\n",
        "        max_seq_length=config.max_length,\n",
        "        dataset_text_field=dataset_text_field\n",
        "    )\n",
        "\n",
        "  # Set supervised fine-tuning parameters\n",
        "  trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        peft_config=peft_config,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"val\"],\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_arguments\n",
        "  )\n",
        "\n",
        "  # Finetune the model\n",
        "  trainer.train()\n",
        "\n",
        "  # Save the model locally\n",
        "  trainer.model.save_pretrained(config.local_save_path)\n",
        "\n",
        "  return trainer.model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to upload the finetuned model to HuggingFace"
      ],
      "metadata": {
        "id": "KZxVvmW9wpK0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBWC2DWNY35m"
      },
      "outputs": [],
      "source": [
        "def upload_model_to_huggingface(finetuned_model, config, tokenizer=None, peft=False, quantized=False):\n",
        "  \"\"\"\n",
        "  Uploads a fine-tuned model and tokenizer to the Hugging Face Hub\n",
        "\n",
        "  Parameters\n",
        "  ==========\n",
        "  finetuned_model (PreTrainedModel): The fine-tuned model to be uploaded.\n",
        "  config (object): Configuration object containing required settings such as repo_user_id and model_name.\n",
        "  tokenizer (PreTrainedTokenizer): The tokenizer associated with the model.\n",
        "  peft (bool): Indicates if the model uses Parameter-Efficient Fine-Tuning (PEFT).\n",
        "  quantized (bool): Indicates if the model is quantized.\n",
        "\n",
        "  Returns\n",
        "  =======\n",
        "  None: The function uploads the model and tokenizer directly to the Hugging Face Hub.\n",
        "  \"\"\"\n",
        "  # Handle PEFT models without quantization\n",
        "  if peft and not quantized:\n",
        "      # Push only the adapter weights to the Hugging Face Hub\n",
        "      finetuned_model.push_to_hub(\n",
        "          f\"{config.repo_user_id}/{config.model_name}_adapter\",\n",
        "          safe_serialization=True,\n",
        "          max_shard_size='3GB'\n",
        "      )\n",
        "      # Merge the adapter weights into the base model for full model upload\n",
        "      model = finetuned_model.merge_and_unload()\n",
        "\n",
        "  # Handle PEFT models with quantization\n",
        "  elif peft and quantized:\n",
        "      # Push the adapter weights to the Hugging Face Hub\n",
        "      finetuned_model.push_to_hub(\n",
        "          f\"{config.repo_user_id}/{config.model_name}_adapter\",\n",
        "          safe_serialization=True,\n",
        "          max_shard_size='3GB'\n",
        "      )\n",
        "      # Load the base model with quantization\n",
        "      base_model, tokenizer = load_model_and_tokenizer(\n",
        "          config, quantization=quantized, add_pad_token=True\n",
        "      )\n",
        "      # Load the adapter and merge it into the base model\n",
        "      model = PeftModel.from_pretrained(\n",
        "          model=base_model,\n",
        "          model_id=f\"{config.repo_user_id}/{config.model_name}_adapter\"\n",
        "      )\n",
        "      model = model.merge_and_unload()\n",
        "\n",
        "  # For models without PEFT or quantization\n",
        "  else:\n",
        "      model = finetuned_model\n",
        "\n",
        "  # Push the final merged model to the Hugging Face Hub\n",
        "  model.push_to_hub(\n",
        "      f\"{config.repo_user_id}/{config.model_name}\",\n",
        "      safe_serialization=True,\n",
        "      max_shard_size='5GB'\n",
        "  )\n",
        "\n",
        "  # Push the tokenizer to the Hugging Face Hub if provided\n",
        "  if tokenizer:\n",
        "      tokenizer.push_to_hub(f\"{config.repo_user_id}/{config.model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataclass to hold the key parameters required while finetuning the models"
      ],
      "metadata": {
        "id": "mu-95Y1cxoCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "  checkpoint:str = \"microsoft/deberta-v3-base\"\n",
        "  max_length:int = 1024\n",
        "  num_labels:int = 6\n",
        "  lora_rank:int = 8\n",
        "  lora_alpha:int = 32\n",
        "  lora_dropout:float = 0.1\n",
        "  lora_bias:str = \"none\"\n",
        "  device:str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
        "  repo_user_id:str = \"bhujith10\"\n",
        "  model_name:str = \"\"\n",
        "  local_save_path:str = \"\"\n",
        "  bf16:bool = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "  fp16:bool = torch.cuda.is_available() and not torch.cuda.is_bf16_supported()\n",
        "  load_in_4bit:bool = True\n",
        "  bnb_4bit_quant_type:bool = \"nf4\"\n",
        "  bnb_4bit_compute_dtype:bool = \"float16\"\n",
        "  bnb_4bit_use_double_quant:bool = False\n",
        "  num_train_epochs:int = 1\n",
        "  batch_size:int = 4\n",
        "  gradient_accumulation_steps:int = 2\n",
        "  gradient_checkpointing:bool = True\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "d0kBT0vwvfcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning\n",
        "\n",
        "This is the entrypoint where we load the dataset, model, finetune the model and upload it to HuggingFace."
      ],
      "metadata": {
        "id": "rmG2Zxp80NRE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8WTrYRnY35m"
      },
      "outputs": [],
      "source": [
        "# Set the prefered base model name\n",
        "checkpoint = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# Update parameters such as model_save_path and checkpoint with the base model name\n",
        "config = update_model_related_settings(checkpoint, config)\n",
        "\n",
        "# Load the model and tokenizer from HuggingFace\n",
        "model, tokenizer = load_model_and_tokenizer(config,\n",
        "                                            add_pad_token=True,\n",
        "                                            quantization=True\n",
        "                                           )\n",
        "\n",
        "# Load the processed dataset from HuggingFace\n",
        "ds = load_dataset('bhujith10/multi_class_classification_dataset')\n",
        "\n",
        "# Format the dataset with the respective model chat template\n",
        "ds = ds.map(format_text,\n",
        "            fn_kwargs={\"tokenizer\": tokenizer},\n",
        "            #remove_columns=column_names,\n",
        "            desc=\"Applying chat template\")\n",
        "\n",
        "sampled_train = ds['train'].shuffle(seed=42).select(range(5000))\n",
        "sampled_val = ds['val'].shuffle(seed=42).select(range(len(ds['val'])))\n",
        "sampled_test = ds['test'].shuffle(seed=42).select(range(len(ds['test'])))\n",
        "\n",
        "# Combine the sampled datasets into one DatasetDict\n",
        "sampled_dataset = DatasetDict({\n",
        "    'train': sampled_train,\n",
        "    'val': sampled_val,\n",
        "    'test': sampled_test\n",
        "})\n",
        "\n",
        "# Finetune the model\n",
        "finetuned_model = finetune_model(model, tokenizer, sampled_dataset, config, peft=True, dataset_text_field='formatted_text')\n",
        "\n",
        "# Upload the model to HuggingFace\n",
        "upload_model_to_huggingface(finetuned_model, config, tokenizer, peft=True, quantized=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goygVRKKY35m",
        "outputId": "e8d04146-08c0-46b9-a3a1-6d62f5fd325e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4544008192, 3407872, 5346.015625)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calculate_model_size(finetuned_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL7lHhu4Y35m"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Make predictions using the finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgSwfG7NY35n"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"bhujith10/Meta-Llama-3.1-8B-Instruct_instruction_tuned_2024_12_05_13_48_adapter\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, add_prefix_space=True)\n",
        "\n",
        "model, tokenizer = load_model_and_tokenizer(config,\n",
        "                                            add_pad_token=False,\n",
        "                                            use_cache=True,\n",
        "                                            quantization=False)\n",
        "\n",
        "model.to('cuda')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COIUuvS5Y35n",
        "outputId": "e33075d4-f564-46ea-ca17-29f7a566ad24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4545335296, 0, 5354.5390625)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calculate_model_size(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQwUYhbHY35n"
      },
      "outputs": [],
      "source": [
        "# Apply the format_text_for_inference function to the test dataset\n",
        "test_ds = ds['test'].map(\n",
        "    format_text_for_inference,\n",
        "    fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    desc=\"Applying chat template\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run inference on the test dataset in batches"
      ],
      "metadata": {
        "id": "7gv5xUU54ELL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_9HtFMWY35o"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "dataloader = DataLoader(test_ds, batch_size=batch_size)\n",
        "\n",
        "# Initialize lists to store actual and generated subjects\n",
        "actual_subjects = []\n",
        "generated_texts = []\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        print(f\"Batch {i}\")\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            batch['formatted_text'],\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=1024,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Move input tensors (input IDs and attention mask) to the GPU for faster computation\n",
        "        tmp_batch = {key: inputs[key].to(config.device) for key in ['input_ids', 'attention_mask']}\n",
        "\n",
        "        outputs = model.generate(**tmp_batch, max_new_tokens=256)\n",
        "\n",
        "        generated_text = [\n",
        "            tokenizer.decode(output[input.shape[-1]:], skip_special_tokens=True)\n",
        "            for input, output in zip(inputs['input_ids'], outputs)\n",
        "        ]\n",
        "\n",
        "        generated_texts.extend(generated_text)\n",
        "        actual_subjects.extend([return_subjects(label) for label in batch['labels']])\n",
        "\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ytSEL81Y35o",
        "outputId": "7d55cc79-346c-4038-a825-5e9da49b22ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1628.9085702896118\n"
          ]
        }
      ],
      "source": [
        "print(end_time - start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store the responses as csv"
      ],
      "metadata": {
        "id": "OOoBwRS-4Rao"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIs-Nh9TY35o"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({'generated_texts_mistral_7b':generated_texts}).to_csv('mistral_7b_raw_generated_outputs.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract predicted subjects list from the response"
      ],
      "metadata": {
        "id": "05nciW9m4XAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import ast\n",
        "\n",
        "# Read the csv file which contains the raw responses\n",
        "raw_responses_df = pd.read_csv('/content/qwen_finetuned_7b_raw_generated_outputs.csv')\n",
        "raw_responses_df.columns = ['text']\n",
        "\n",
        "# Apply the function on the responses to extract the predicted subjects list\n",
        "raw_responses_df['predicted_labels'] = raw_responses_df['text'].apply(extract_list_from_raw_response)\n",
        "\n",
        "# Calculate F1 scores\n",
        "predicted_labels = raw_responses_df['predicted_labels'].to_list()\n",
        "actual_labels = ds['test']['labels']\n",
        "\n",
        "calculate_f1_score(actual_labels,predicted_labels)"
      ],
      "metadata": {
        "id": "gSrbDjtyY87F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate F1 scores only for the valid responses\n",
        "\n",
        "Valid responses are the ones which contain the predicted subjects in list format."
      ],
      "metadata": {
        "id": "SLBOyYG74tmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modified_predicted_labels = []\n",
        "modified_actual_labels = []\n",
        "\n",
        "for i in range(len(predicted_labels)):\n",
        "  if predicted_labels[i] != [0, 0, 0, 0, 0, 0]:\n",
        "    modified_predicted_labels.append(predicted_labels[i])\n",
        "    modified_actual_labels.append(actual_labels[i])\n",
        "\n",
        "calculate_f1_score(modified_actual_labels,modified_predicted_labels)"
      ],
      "metadata": {
        "id": "T6Hx9eKtY8vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuvQh7TsY35o"
      },
      "outputs": [],
      "source": [
        "# Use this command to find the largest files in the disk\n",
        "# !find / -type f -exec du -h {} + | sort -rh | head -n 10\n",
        "\n",
        "# Use this command to delete it\n",
        "# !rm -r /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/blobs/92ecfe1a2414458b4821ac8c13cf8cb70aed66b5eea8dc5ad9eeb4ff309d6d7b"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}